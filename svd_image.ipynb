{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOwmWIml1Nv8/73b32IMtSL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iceman67/kaeri/blob/main/svd_image.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVD\n",
        "\n",
        "* Singular Value Decomposition (SVD) is a powerful matrix factorization technique that has numerous applications in data analysis  to help distinguish patterns within a datase\n",
        "\n",
        "* Matrix Decomposition:\n",
        "SVD decomposes a matrix (let's call it \"A\") into three other matrices: U, $\\Sigma$, and $V^ᵀ$\n",
        "\n",
        "\n",
        "A = U  $\\Sigma$  $V^ᵀ$\n",
        "\n",
        "where,\n",
        "U: A left singular matrix.\n",
        "$\\Sigma$ : A diagonal matrix containing singular values.\n",
        "$V^ᵀ$: The transpose of the right singular matrix.\n",
        "\n",
        "\n",
        "* SVD helps reveal underlying patterns in data by decomposing it into its constituent components\n",
        "\n",
        "\n",
        "* Feature Extraction:\n",
        "SVD can be used to extract meaningful features from data.\n",
        "These features can then be used for tasks such as classification, clustering, or anomaly detection.\n",
        "\n",
        "\n",
        "\n",
        "* SVD allows you to break down complex data into fundamental components, revealing significant patterns, filtering out noise, and enabling dimensionality reduction.\n",
        "\n",
        "\n",
        "## Feature Extraction using SVD\n",
        "\n",
        "* Imagine your dataset as a matrix. Rows represent individual data points (e.g., images, documents, user profiles), and columns represent features (e.g., pixel values, word frequencies, ratings)\n",
        "\n",
        "* SVD decomposes this matrix into three matrices: U, Σ, and Vᵀ.\n",
        "The key here is the Σ matrix, which contains the $singular\\  values$, and the U and V matrices, which contain the singular vectors\n",
        "\n",
        "* Selecting Significant Components:\n",
        "Sort the singular values in descending order.\n",
        "Choose the top 'k' singular values, where 'k' is a smaller number than the original number of features. This is where dimensionality reduction happens.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OXXnZ-5ccDJp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* USPS dataset\n",
        "> https://www.kaggle.com/bistaumanga/usps-dataset\n",
        "\n",
        "* SVD 이미지\n",
        "> https://towardsdatascience.com/how-to-use-singular-value-decomposition-svd-for-image-classification-in-python-20b1b2ac4990\n"
      ],
      "metadata": {
        "id": "k6-jXjrVbK1Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The product $C=AB$ of two matrices $A (n×m)$ and $B (m×p)$ should have a shape of $n×p$.\n",
        "\n",
        "> https://towardsdatascience.com/pca-and-svd-explained-with-numpy-5d13b0d2a4d8\n",
        "\n",
        "\n",
        "Matrix decomposition methods usually break single matrices down into a product of matrices, which offer advantages in a range of problems.\n",
        "\n",
        "\n",
        "특이값 분해(Singular Value Decomposition)\n",
        "*  고유값 분해의 일반화 버전"
      ],
      "metadata": {
        "id": "rzzk24nIOmFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "A = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
        "U,s,VT = np.linalg.svd(A)"
      ],
      "metadata": {
        "id": "oAWMd_YO_T1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (f\"U = {U}\")\n",
        "print (f\"S = {s}\")\n",
        "print (f\"VT = {VT}\")"
      ],
      "metadata": {
        "id": "qjscbp_FP_KW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A_remake = (U @ np.diag(s) @ VT)\n",
        "print(A_remake)"
      ],
      "metadata": {
        "id": "q9ve0rURFNDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.allclose(A, U @ np.diag(s) @ VT)"
      ],
      "metadata": {
        "id": "XRkswps1sFeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s.size"
      ],
      "metadata": {
        "id": "HEcaVaesGXA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate random vector and send A  safely with the vector"
      ],
      "metadata": {
        "id": "lSkAa7CCHFpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "shapeA = A.shape\n",
        "rand_vector = np.random.randint(0, 255, size=(shapeA[0], shapeA[1]))\n",
        "print(rand_vector)"
      ],
      "metadata": {
        "id": "HLnWlwj9Gdtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A_hat =np.bitwise_xor(A, rand_vector)\n",
        "A =np.bitwise_xor(A_hat, rand_vector)"
      ],
      "metadata": {
        "id": "ZERoMvO6HKRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A"
      ],
      "metadata": {
        "id": "Fw3I9U2SKXSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "A = np.matrix([[1, 0.3], [0.45, 1.2]])\n",
        "U, s, VT = np.linalg.svd(A)"
      ],
      "metadata": {
        "id": "VuvmHvPFr3VV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.allclose(A, U * np.diag(s) * VT)"
      ],
      "metadata": {
        "id": "ehMIYpqwr-2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Singular Value $S$ is ordered from big to small\n",
        "* $V^T$ is transposed, you can caculate $A$ as:"
      ],
      "metadata": {
        "id": "gyG4XqJ9CuTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sima를 다시 0 을 포함한 대칭행렬로 변환\n",
        "Sigma_mat = np.diag(s)\n",
        "a_ = np.dot(np.dot(U, Sigma_mat), VT)\n",
        "print(np.round(a_, 3))"
      ],
      "metadata": {
        "id": "3ahYR19h_hd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "A = np.mat([[1,2,3,4],[5,6,7,8],[2,3,4,5]])\n",
        "u, s, vt = np.linalg.svd(A)\n",
        "print(A.shape)\n",
        "print(u.shape)\n",
        "print(s.shape)\n",
        "print(vt.shape)\n",
        "\n",
        "print(u.dot(np.column_stack((np.diag(s), np.zeros(3))).dot(vt)))"
      ],
      "metadata": {
        "id": "RG7hdWXEAmAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# numpy 모듈을 np라는 별명으로 불러온다.\n",
        "import numpy as np\n",
        "\n",
        "# 행렬 A는 이전 포스트에 있는 예제에서 사용된 것이다.\n",
        "A = [[i**2, i, 1] for i in range(1930, 2020, 10)]\n",
        "\n",
        "# 행렬 A를 numpy 함수들이 다룰 수 있는 numpy array으로 변환한다.\n",
        "matA = np.array(A).astype(np.float64)\n",
        "\n",
        "print (matA.shape) # 3 x 9\n",
        "\n",
        "# svd함수를 사용하여 3개의 반환값(U,s,V)를 저장한다.\n",
        "U, s, V = np.linalg.svd(matA, full_matrices = True)\n",
        "\n",
        "# s는 matA의 고유값(eigenvalue) 리스트이다.\n",
        "# svd를 이용하여 근사한(approximated) 결과를 원본과 비교하기 위해\n",
        "# s를 유사대각행렬로 변환한다.\n",
        "# 유사행렬에 대한 내용은 이전 포스트 참조.\n",
        "S = np.zeros(matA.shape)\n",
        "for i in range(len(s)):\n",
        "     S[i][i] = s[i]\n",
        "\n",
        "# 근사한 결과를 계산한다.\n",
        "appA = np.dot(U, np.dot(S, V))\n",
        "\n",
        "# 원래 행렬인 matA와 근사한 행렬인 appA가 서로 비슷하다면\n",
        "# 두 행렬의 차이는 영행렬(zero matrix)에 가까울 것이다.\n",
        "# 즉, 다음의 결과가 대부분 0으로 채워져있다면 성공적인 svd가 이루어진 것이다.\n",
        "# 파이썬 버전에 따라 출력방식이 다르므로 주의한다.\n",
        "print(matA - appA)"
      ],
      "metadata": {
        "id": "MQuMNmSN94DH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# numpy의 svd 모듈 import\n",
        "import numpy as np\n",
        "from numpy.linalg import svd\n",
        "\n",
        "# 4X4 Random 행렬 a 생성\n",
        "np.random.seed(121)\n",
        "a = np.random.randn(4,4)\n",
        "print(np.round(a, 3))"
      ],
      "metadata": {
        "id": "ZvsOqnR9MOdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "U, Sigma, Vt = svd(a)\n",
        "print(U.shape, Sigma.shape, Vt.shape)\n",
        "print('U matrix:\\n',np.round(U, 3))\n",
        "print('Sigma Value:\\n',np.round(Sigma, 3))\n",
        "print('V transpose matrix:\\n',np.round(Vt, 3))"
      ],
      "metadata": {
        "id": "Q7UhBKbNMXa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sima를 다시 0 을 포함한 대칭행렬로 변환\n",
        "Sigma_mat = np.diag(Sigma)\n",
        "a_ = np.dot(np.dot(U, Sigma_mat), Vt)\n",
        "print(np.round(a_, 3))"
      ],
      "metadata": {
        "id": "PJUQ6TRaMjp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.sparse.linalg import svds\n",
        "from scipy.linalg import svd\n",
        "\n",
        "# 원본 행렬을 출력하고, SVD를 적용할 경우 U, Sigma, Vt 의 차원 확인\n",
        "np.random.seed(121)\n",
        "matrix = np.random.random((6, 6))\n",
        "print('원본 행렬:\\n',matrix)\n",
        "U, Sigma, Vt = svd(matrix, full_matrices=False)\n",
        "print('\\n분해 행렬 차원:',U.shape, Sigma.shape, Vt.shape)\n",
        "print('\\nSigma값 행렬:', Sigma)\n",
        "\n",
        "# Truncated SVD로 Sigma 행렬의 특이값을 4개로 하여 Truncated SVD 수행.\n",
        "num_components = 4\n",
        "U_tr, Sigma_tr, Vt_tr = svds(matrix, k=num_components)\n",
        "print('\\nTruncated SVD 분해 행렬 차원:',U_tr.shape, Sigma_tr.shape, Vt_tr.shape)\n",
        "print('\\nTruncated SVD Sigma값 행렬:', Sigma_tr)\n",
        "matrix_tr = np.dot(np.dot(U_tr,np.diag(Sigma_tr)), Vt_tr)  # output of TruncatedSVD\n",
        "\n",
        "print('\\nTruncated SVD로 분해 후 복원 행렬:\\n', matrix_tr)\n"
      ],
      "metadata": {
        "id": "5n3ZV7DbM5Oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying SVD onto acoustic sensor data"
      ],
      "metadata": {
        "id": "ukIG6RyrQfVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "id": "wF3oPk4JLWF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/content/gdrive/MyDrive/kaeri/\""
      ],
      "metadata": {
        "id": "fAIvM6JcLwk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.sparse.linalg import svds\n",
        "from scipy.linalg import svd\n",
        "\n",
        "normal_matrix = np.load(data_dir +'normal_data.npy') # load\n"
      ],
      "metadata": {
        "id": "tD_jzufsQe-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normal_matrix.shape"
      ],
      "metadata": {
        "id": "R88JnPPRRzQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import svd\n",
        "\n",
        "def find_important_features_svd(data, top_k=None):\n",
        "    \"\"\"\n",
        "    Finds important features using Singular Value Decomposition (SVD).\n",
        "\n",
        "    Args:\n",
        "        data (numpy.ndarray): The data matrix (m x n).\n",
        "        top_k (int, optional): Number of top features to return. If None, returns all.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Array of feature importance scores.\n",
        "        list: List of the top k feature indices.\n",
        "    \"\"\"\n",
        "    U, Sigma, VT = svd(data)\n",
        "    V = VT.T\n",
        "\n",
        "    # Calculate feature importance scores\n",
        "    feature_importance = np.abs(V) @ np.diag(Sigma)\n",
        "    feature_importance_summed = np.sum(feature_importance, axis=1)\n",
        "\n",
        "    # Get the indices of the most important features\n",
        "    feature_indices_sorted = np.argsort(feature_importance_summed)[::-1]\n",
        "\n",
        "    if top_k is not None:\n",
        "        top_feature_indices = feature_indices_sorted[:top_k]\n",
        "        top_feature_importance = feature_importance_summed[top_feature_indices]\n",
        "        return feature_importance_summed, top_feature_indices\n",
        "    else:\n",
        "        return feature_importance_summed, feature_indices_sorted"
      ],
      "metadata": {
        "id": "6WvvlE_bToJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importance_scores, top_feature_indices = find_important_features_svd(normal_matrix, top_k=2)\n",
        "\n",
        "print(\"Feature Importance Scores:\", feature_importance_scores)\n",
        "print(\"Top Feature Indices:\", top_feature_indices)\n",
        "\n",
        "feature_importance_scores_all, feature_indices_sorted_all = find_important_features_svd(normal_matrix)\n",
        "print(\"All feature importance scores sorted:\", feature_importance_scores_all)\n",
        "print(\"All feature indices sorted:\", feature_indices_sorted_all)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "14wir8t2TuE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#def plot_average_signals(normal_data, leak_data=None):\n",
        "\n",
        "def plot_average_signals(normal_data, title):\n",
        "\n",
        "    \"\"\"정상 데이터와 누수 데이터의 평균 신호를 시각화합니다.\n",
        "\n",
        "    Args:\n",
        "        normal_data: 정상 데이터 (numpy array)\n",
        "        leak_data: 누수 데이터 (numpy array)\n",
        "    \"\"\"\n",
        "    # 각 데이터의 평균 계산\n",
        "    normal_mean = np.mean(normal_data, axis=0)\n",
        "    normal_stddev = np.std(normal_data, axis=0)\n",
        "  #  leak_mean = np.mean(leak_data, axis=0)\n",
        "\n",
        "    # 시각화\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    time_steps = np.arange(len(normal_mean))\n",
        "\n",
        "    # 정상 데이터 (파란색 실선)\n",
        "    plt.plot(time_steps, normal_mean, \"b-\", label=f\"{title} Mean\", linewidth=2)\n",
        "\n",
        "    # 누수 데이터 (빨간색 점선)\n",
        "    plt.plot(time_steps, normal_stddev, \"r--\", label=f\"{title} Std Dev\", linewidth=2)\n",
        "\n",
        "    plt.title(f\"Average Signal Comparison: {title}\")\n",
        "    plt.xlabel(\"Fequency\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# feature_indices_sorted_all"
      ],
      "metadata": {
        "id": "hMJb2S3pUlYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_average_signals(normal_matrix, \"Normal\")"
      ],
      "metadata": {
        "id": "tuWpP0laUnAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/content/gdrive/MyDrive/kaeri/\"\n",
        "leak_matrix = np.load(data_dir+ 'leak_data.npy') # load"
      ],
      "metadata": {
        "id": "cmIJFwWHe_rS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "leak_matrix.shape"
      ],
      "metadata": {
        "id": "QfaFwuN5ySIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_average_signals(leak_matrix, \"Leak\")"
      ],
      "metadata": {
        "id": "zLd2MlUOfNH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importance_scores, top_feature_indices = find_important_features_svd(leak_matrix, top_k=2)\n",
        "\n",
        "print(\"Feature Importance Scores:\", feature_importance_scores)\n",
        "print(\"Top Feature Indices:\", top_feature_indices)\n",
        "\n",
        "feature_importance_scores_all, feature_indices_sorted_all = find_important_features_svd(leak_matrix)\n",
        "print(\"All feature importance scores sorted:\", feature_importance_scores_all)\n",
        "print(\"All feature indices sorted:\", feature_indices_sorted_all)"
      ],
      "metadata": {
        "id": "7zO5eS8tgOqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "normal_matrix = np.load(data_dir + 'normal_data.npy') # load\n",
        "leak_matrix = np.load( data_dir + 'leak_data.npy') # load\n",
        "\n",
        "all_matrix = np.concatenate((normal_matrix, leak_matrix), axis=0)"
      ],
      "metadata": {
        "id": "olP8vcJlfnGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " plot_average_signals(all_matrix, \"Normal+Leak\")"
      ],
      "metadata": {
        "id": "e99jwL_5f07x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importance_scores, top_feature_indices = find_important_features_svd(all_matrix, top_k=2)\n",
        "\n",
        "print(\"Feature Importance Scores:\", feature_importance_scores)\n",
        "print(\"Top Feature Indices:\", top_feature_indices)\n",
        "\n",
        "feature_importance_scores_all, feature_indices_sorted_all = find_important_features_svd(all_matrix)\n",
        "print(\"All feature importance scores sorted:\", feature_importance_scores_all)\n",
        "print(\"All feature indices sorted:\", feature_indices_sorted_all)"
      ],
      "metadata": {
        "id": "Z1YjTZpl7c0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_indices_sorted_all"
      ],
      "metadata": {
        "id": "j2KsBAeXCWdo",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importance_scores_all"
      ],
      "metadata": {
        "id": "AH1c3sij_jAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the feature importance scores\n",
        "plt.figure(figsize=(17, 6))\n",
        "plt.bar(range(len(feature_importance_scores_all)), feature_importance_scores_all, label=\"Importance Features\")\n",
        "plt.xlabel(\"Feature Index\")\n",
        "plt.ylabel(\"Feature Importance Score\")\n",
        "plt.title(\"(Normal + Leak)Feature Importance Scores from SVD\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "#plt.xticks(range(len(feature_importance_scores_all)))  # Show all feature indices on x-axis\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qn2pZXDh6wtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (Normal + Leak) The result of SVD"
      ],
      "metadata": {
        "id": "3LOFBcVVkClu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "U, Sigma, Vt = svd(all_matrix, full_matrices=False)\n",
        "print('\\n분해 행렬 차원:',U.shape, Sigma.shape, Vt.shape)\n",
        "print('\\nSigma값 행렬:', Sigma)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RYAc3S2KR7gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Truncated SVD로 Sigma 행렬의 특이값을 4개로 하여 Truncated SVD 수행.\n",
        "num_components = 4\n",
        "U_tr, Sigma_tr, Vt_tr = svds(all_matrix, k=num_components)\n",
        "print('\\nTruncated SVD 분해 행렬 차원:',U_tr.shape, Sigma_tr.shape, Vt_tr.shape)\n",
        "print('\\nTruncated SVD Sigma값 행렬:', Sigma_tr)\n",
        "matrix_tr = np.dot(np.dot(U_tr,np.diag(Sigma_tr)), Vt_tr)  # output of TruncatedSVD\n",
        "\n",
        "print('\\nTruncated SVD로 분해 후 복원 행렬:\\n', matrix_tr)"
      ],
      "metadata": {
        "id": "d6r2k-u6SFiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SVD 의 일부로 부터 이미지를 재구성함"
      ],
      "metadata": {
        "id": "BIQHQX6emf7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "reconstimg = np.matrix(U[:, :1]) * np.diag(Sigma[:1]) * np.matrix(Vt[:1, :])\n",
        "\n",
        "plt.imshow(reconstimg, cmap=\"inferno\", interpolation=\"bilinear\")\n",
        "plt.colorbar()  # Add a colorbar to show the value mapping\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9DzAbbJBSa4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(all_matrix, cmap=\"inferno\", interpolation=\"bilinear\")\n",
        "plt.colorbar()  # Add a colorbar to show the value mapping\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7YlO_sdZDsTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def matrix_difference(matrix1, matrix2, absolute=True, squared=False):\n",
        "    \"\"\"\n",
        "    Calculates the element-wise difference between two matrices.\n",
        "\n",
        "    Args:\n",
        "        matrix1 (numpy.ndarray): The first matrix.\n",
        "        matrix2 (numpy.ndarray): The second matrix.\n",
        "        absolute (bool): If True, returns the absolute difference.\n",
        "        squared (bool): If True, returns the squared difference.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: The matrix of element-wise differences.\n",
        "    \"\"\"\n",
        "\n",
        "    matrix1 = np.array(matrix1)\n",
        "    matrix2 = np.array(matrix2)\n",
        "\n",
        "    if matrix1.shape != matrix2.shape:\n",
        "        raise ValueError(\"Matrices must have the same shape.\")\n",
        "\n",
        "    diff = matrix1 - matrix2\n",
        "\n",
        "    if squared:\n",
        "        diff = diff**2\n",
        "    elif absolute:\n",
        "        diff = np.abs(diff)\n",
        "\n",
        "    return diff\n",
        "\n",
        "def total_difference(matrix1, matrix2, absolute=True, squared=False, average=False):\n",
        "    \"\"\"\n",
        "    Calculates the total difference between two matrices.\n",
        "\n",
        "    Args:\n",
        "        matrix1 (numpy.ndarray): The first matrix.\n",
        "        matrix2 (numpy.ndarray): The second matrix.\n",
        "        absolute (bool): If True, returns the absolute difference.\n",
        "        squared (bool): If True, returns the squared difference.\n",
        "        average (bool): If True, returns the average of the differences. Otherwise returns the sum.\n",
        "\n",
        "    Returns:\n",
        "        float: The total (or average) difference between the matrices.\n",
        "    \"\"\"\n",
        "    diff_matrix = matrix_difference(matrix1, matrix2, absolute, squared)\n",
        "    if average:\n",
        "        return np.mean(diff_matrix)\n",
        "    else:\n",
        "        return np.sum(diff_matrix)"
      ],
      "metadata": {
        "id": "yl9zyQriFED7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Element-wise absolute difference\n",
        "element_diff = matrix_difference(all_matrix, reconstimg)\n",
        "print(\"Element-wise absolute difference:\\n\", element_diff)\n",
        "\n",
        "# Element-wise squared difference\n",
        "element_squared_diff = matrix_difference(all_matrix, reconstimg, absolute=False, squared=True)\n",
        "print(\"\\nElement-wise squared difference:\\n\", element_squared_diff)\n",
        "\n",
        "# Total absolute difference\n",
        "total_abs_diff = total_difference(all_matrix, reconstimg)\n",
        "print(\"\\nTotal absolute difference:\", total_abs_diff)\n",
        "\n",
        "# Total squared difference\n",
        "total_squared_diff = total_difference(all_matrix, reconstimg, absolute=False, squared=True)\n",
        "print(\"\\nTotal squared difference:\", total_squared_diff)\n",
        "\n",
        "# Average absolute difference\n",
        "average_abs_diff = total_difference(all_matrix, reconstimg, average=True)\n",
        "print(\"\\nAverage absolute difference:\", average_abs_diff)\n",
        "\n",
        "# Average squared difference\n",
        "average_squared_diff = total_difference(all_matrix, reconstimg, absolute=False, squared=True, average=True)\n",
        "print(\"\\nAverage squared difference:\", average_squared_diff)\n"
      ],
      "metadata": {
        "id": "bPKHa-leFGgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(2, 5):\n",
        "    reconstimg = np.matrix(U[:, :i]) * np.diag(Sigma[:i]) * np.matrix(Vt[:i, :])\n",
        "    plt.imshow(reconstimg, cmap=\"inferno\", interpolation=\"bilinear\")\n",
        "    plt.colorbar()  # Add a colorbar to show the value mapping\n",
        "    title = \"n = %s\" % i\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "dDb8TDZRTDuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def find_significant_components(matrix, num_components=None, threshold=None):\n",
        "    \"\"\"\n",
        "    Finds significant components in a matrix using Singular Value Decomposition (SVD).\n",
        "\n",
        "    Args:\n",
        "        matrix (numpy.ndarray): The input matrix.\n",
        "        num_components (int, optional): The number of top components to keep.\n",
        "        threshold (float, optional): A threshold value for singular values.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - U_reduced (numpy.ndarray): The reduced left singular matrix.\n",
        "            - Sigma_reduced (numpy.ndarray): The reduced singular value matrix (diagonal).\n",
        "            - V_reduced (numpy.ndarray): The reduced right singular matrix.\n",
        "    \"\"\"\n",
        "\n",
        "    U, Sigma, VT = np.linalg.svd(matrix)\n",
        "\n",
        "    if num_components is not None:\n",
        "        # Select top num_components\n",
        "        U_reduced = U[:, :num_components]\n",
        "        Sigma_reduced = np.diag(Sigma[:num_components])\n",
        "        VT_reduced = VT[:num_components, :]\n",
        "\n",
        "    elif threshold is not None:\n",
        "        # Select components based on threshold\n",
        "        significant_indices = np.where(Sigma > threshold)[0]\n",
        "        U_reduced = U[:, significant_indices]\n",
        "        Sigma_reduced = np.diag(Sigma[significant_indices])\n",
        "        VT_reduced = VT[significant_indices, :]\n",
        "    else:\n",
        "        # Return all components\n",
        "        U_reduced = U\n",
        "        Sigma_reduced = np.diag(Sigma)\n",
        "        VT_reduced = VT\n",
        "\n",
        "    return U_reduced, Sigma_reduced, VT_reduced\n",
        "\n",
        "\n",
        "# Find top 2 significant components\n",
        "U_reduced_2, Sigma_reduced_2, VT_reduced_2 = find_significant_components(all_matrix, num_components=2)\n",
        "print(\"Reduced matrices (top 2 components):\")\n",
        "print(\"U_reduced:\\n\", U_reduced_2)\n",
        "print(\"Sigma_reduced:\\n\", Sigma_reduced_2)\n",
        "print(\"VT_reduced:\\n\", VT_reduced_2)\n",
        "\n",
        "# Find components with singular values above a threshold\n",
        "threshold = 10.0\n",
        "U_reduced_threshold, Sigma_reduced_threshold, VT_reduced_threshold = find_significant_components(all_matrix, threshold=threshold)\n",
        "\n",
        "print(\"\\nReduced matrices (threshold = 10.0):\")\n",
        "print(\"U_reduced:\\n\", U_reduced_threshold)\n",
        "print(\"Sigma_reduced:\\n\", Sigma_reduced_threshold)\n",
        "print(\"VT_reduced:\\n\", VT_reduced_threshold)\n",
        "\n",
        "# Find all components\n",
        "U_all, Sigma_all, VT_all = find_significant_components(all_matrix)\n",
        "\n",
        "print(\"\\nAll Matrices:\")\n",
        "print(\"U:\\n\", U_all)\n",
        "print(\"Sigma:\\n\", np.diag(Sigma_all)) # Show the diagonal of sigma\n",
        "print(\"VT:\\n\", VT_all)\n",
        "\n",
        "#Example with random data\n",
        "random_matrix = np.random.rand(10, 5)\n",
        "U_rand, Sigma_rand, VT_rand = find_significant_components(all_matrix, num_components = 3)\n",
        "print(\"\\nRandom Matrix Reduced:\")\n",
        "print(\"U:\\n\", U_rand)\n",
        "print(\"Sigma:\\n\", Sigma_rand)\n",
        "print(\"VT:\\n\", VT_rand)"
      ],
      "metadata": {
        "id": "zD32NJBFevZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# 코드로 형식 지정됨\n",
        "```\n",
        "\n",
        "## End Applying SVD"
      ],
      "metadata": {
        "id": "W02tI529Rbfj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying supervised ML including SVM using sensor data\n",
        "\n",
        "\n",
        "```\n",
        "# 코드로 형식 지정됨\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "dDrdOQ2ykNIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "data = np.load(data_dir+'normal_data.npy', allow_pickle=True)  # 'your_data.npy' 파일 경로를 적절히 수정\n",
        "df_normal = pd.DataFrame(data)\n",
        "df_normal[320]= 1\n",
        "\n",
        "data = np.load(data_dir+'leak_data.npy', allow_pickle=True)  # 'your_data.npy' 파일 경로를 적절히 수정\n",
        "df_leak = pd.DataFrame(data)\n",
        "df_leak[320]= 0\n"
      ],
      "metadata": {
        "id": "NztwzTovkW1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def concat_dataframes(df1, df2, axis=0, join='outer', ignore_index=False):\n",
        "    \"\"\"\n",
        "    Concatenates two pandas DataFrames.\n",
        "\n",
        "    Args:\n",
        "        df1 (pandas.DataFrame): The first DataFrame.\n",
        "        df2 (pandas.DataFrame): The second DataFrame.\n",
        "        axis (int, optional): The axis to concatenate along (0 for rows, 1 for columns). Defaults to 0.\n",
        "        join (str, optional): How to handle indexes on other axis(es) (outer or inner). Defaults to 'outer'.\n",
        "        ignore_index (bool, optional): If True, do not use the index values along the concatenation axis. The resulting axis will be labeled 0, ..., n - 1. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: The concatenated DataFrame.\n",
        "    \"\"\"\n",
        "    return pd.concat([df1, df2], axis=axis, join=join, ignore_index=ignore_index)\n"
      ],
      "metadata": {
        "id": "h8LyKE7FnwAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = concat_dataframes(df_normal, df_leak )\n"
      ],
      "metadata": {
        "id": "OSA_8_DVl1Q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "UILmBEtJKUAx",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.tail()"
      ],
      "metadata": {
        "id": "fyHwAZm2mkEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.iloc[:, 0:-1]\n",
        "y = data.iloc[:, -1]\n"
      ],
      "metadata": {
        "id": "WwRI96RTn5hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.head()"
      ],
      "metadata": {
        "id": "aSfGlsWPpJ7O",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.head()"
      ],
      "metadata": {
        "id": "lPMmzhD_pO4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DBScan"
      ],
      "metadata": {
        "id": "W-n9-75MRKng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "I7DIuIvxRvZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=4)\n",
        "pca.fit(data_scaled)\n",
        "df_pca = pca.transform(data_scaled)\n",
        "print(df_pca.shape)"
      ],
      "metadata": {
        "id": "uuvyUydER6mA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "db = DBSCAN(eps=0.3, min_samples=6).fit(df_pca)\n",
        "labels = db.labels_\n",
        "\n",
        "# Number of clusters in labels, ignoring noise if present.\n",
        "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "n_noise_ = list(labels).count(-1)\n",
        "\n",
        "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
        "print(\"Estimated number of noise points: %d\" % n_noise_)"
      ],
      "metadata": {
        "id": "9hd3934LQ5xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "unique_labels = set(labels)\n",
        "core_samples_mask = np.zeros_like(labels, dtype=bool)\n",
        "core_samples_mask[db.core_sample_indices_] = True\n",
        "\n",
        "colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
        "for k, col in zip(unique_labels, colors):\n",
        "    if k == -1:\n",
        "        # Black used for noise.\n",
        "        col = [0, 0, 0, 1]\n",
        "\n",
        "    class_member_mask = labels == k\n",
        "\n",
        "    xy = df_pca[class_member_mask & core_samples_mask]\n",
        "    plt.plot(\n",
        "        xy[:, 0],\n",
        "        xy[:, 1],\n",
        "        \"o\",\n",
        "        markerfacecolor=tuple(col),\n",
        "        markeredgecolor=\"r\",\n",
        "        markersize=14,\n",
        "    )\n",
        "\n",
        "    xy = df_pca[class_member_mask & ~core_samples_mask]\n",
        "    plt.plot(\n",
        "        xy[:, 0],\n",
        "        xy[:, 1],\n",
        "        \"o\",\n",
        "        markerfacecolor=tuple(col),\n",
        "        markeredgecolor=\"b\",\n",
        "        markersize=6,\n",
        "    )\n",
        "\n",
        "plt.title(f\"Estimated number of clusters: {n_clusters_}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Pr4uy1j5Swfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## End of DBScan"
      ],
      "metadata": {
        "id": "kPX0BdDURNsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # 테스트 데이터 비율 20%, random_state는 시드 값\n"
      ],
      "metadata": {
        "id": "WIKKrKcFpXBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. SVM 모델 생성 및 학습\n",
        "model = SVC(kernel='linear')  # kernel은 'linear', 'poly', 'rbf', 'sigmoid' 등 사용 가능\n",
        "model.fit(X_train, y_train)\n",
        "# 5. 예측\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "LtL0Z9OEphrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. 평가\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\" Classification Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "MmRdoXg1pnnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc_X = StandardScaler()\n",
        "X_trainscaled=sc_X.fit_transform(X_train)\n",
        "X_testscaled=sc_X.transform(X_test)\n",
        "model = SVC(kernel='linear')  # kernel은 'linear', 'poly', 'rbf', 'sigmoid' 등 사용 가능\n",
        "model.fit(X_trainscaled, y_train)\n",
        "y_pred=model.predict(X_testscaled)\n",
        "print(\"The Score with \", (r2_score(y_pred, y_test)))"
      ],
      "metadata": {
        "id": "OWHJOYUfqFWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score"
      ],
      "metadata": {
        "id": "91rdOV6Aqf6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc_X = StandardScaler()\n",
        "X_trainscaled=sc_X.fit_transform(X_train)\n",
        "X_testscaled=sc_X.transform(X_test)\n",
        "\n",
        "reg = MLPRegressor(hidden_layer_sizes=(64,64,64),activation=\"relu\" ,random_state=1, max_iter=2000).fit(X_trainscaled, y_train)\n",
        "\n",
        "y_pred=reg.predict(X_testscaled)\n",
        "print(\"The Score with \", (r2_score(y_pred, y_test)))"
      ],
      "metadata": {
        "id": "QFK9rZxhqkw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_trainscaled, y_train)\n",
        "\n",
        "y_pred=knn.predict(X_testscaled)\n",
        "print(\"The Score with \", (r2_score(y_pred, y_test)))"
      ],
      "metadata": {
        "id": "0gYpK801qyEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    # 한 개의 층으로만 구성\n",
        "    tf.keras.layers.Dense(1, input_dim=320, activation='sigmoid'),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='sgd',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=1, test_size=0.1)\n",
        "sc_X = StandardScaler()\n",
        "X_trainscaled=sc_X.fit_transform(X_train)\n",
        "X_testscaled=sc_X.transform(X_test)\n",
        "\n",
        "hist = model.fit(X_trainscaled, y_train, epochs=200)\n",
        "\n"
      ],
      "metadata": {
        "id": "rtY-OXu1q8ZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (f\"accuracy = {max(hist.history['accuracy'])}\")"
      ],
      "metadata": {
        "id": "qAxNEr9ds4XX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "S-u8MLTAtDVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure (figsize=(8,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.title(\"loss\")\n",
        "plt.subplot(1,2,2)\n",
        "plt.title(\"accuracy\")\n",
        "plt.plot(hist.history['accuracy'], 'b-', label='training')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EtBj8IExtLFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict(X_testscaled)\n",
        "preds_1d = preds.flatten()\n",
        "pred_class = np.where(preds_1d > 0.5, 1 , 0)\n",
        "\n",
        "\n",
        "y_true = X_testscaled[pred_class==1]\n",
        "y_false = X_testscaled[pred_class==0]\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (12,5))\n",
        "ax.plot(y_true[:, 0], y_true[:,1], 'ro')\n",
        "ax.plot(y_false[:,0], y_false[:,1], 'bo')\n",
        "ax.grid()"
      ],
      "metadata": {
        "id": "Sl8TE--jtUGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict(X_testscaled)\n",
        "preds_1d = preds.flatten()\n",
        "pred_class = np.where(preds_1d > 0.5, 1 , 0)\n"
      ],
      "metadata": {
        "id": "A88gYfmTtnHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = y_test.to_numpy()"
      ],
      "metadata": {
        "id": "sSr2NuUPuKyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(y_test)):\n",
        "    if y_test[i] != pred_class[i]:\n",
        "        count = count + 1\n",
        "\n",
        "print (f\"precision = {1 - (count/len(y_test))}\")"
      ],
      "metadata": {
        "id": "mTSxk69-vWSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## END of Applying supervised ML including SVM using sensor data\n"
      ],
      "metadata": {
        "id": "OsXPQ_m1ps5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing required modules\n",
        "import numpy as np\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "#Creating array\n",
        "A = np.array([[3,4,3],[1,2,3],[4,2,1]])\n",
        "\n",
        "#Fitting the SVD class\n",
        "trun_svd =  TruncatedSVD(n_components = 2)\n",
        "A_transformed = trun_svd.fit_transform(A)\n",
        "\n",
        "#Printing the transformed matrix\n",
        "print(\"Transformed Matrix:\")\n",
        "print(A_transformed)"
      ],
      "metadata": {
        "id": "Ggzhlk1eFo-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-jGq7fJWpNbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "c_names = ['post1', 'post2', 'post3', 'post4']\n",
        "words = ['ice', 'snow', 'tahoe', 'goal', 'puck']\n",
        "post_words = pd.DataFrame([[4, 4, 6, 2],\n",
        "                           [6, 1, 0, 5],\n",
        "                           [3, 0, 0, 5],\n",
        "                           [0, 6, 5, 1],\n",
        "                           [0, 4, 5, 0]],\n",
        "                          index = words,\n",
        "                          columns = c_names)\n",
        "post_words.index.names = ['word:']\n",
        "post_words"
      ],
      "metadata": {
        "id": "91sPkv6MziRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "U, sigma, V = np.linalg.svd(post_words)\n",
        "print (\"V = \")\n",
        "print (np.round(V, decimals=2))"
      ],
      "metadata": {
        "id": "b1DN2qF1zwSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(V, interpolation='none')\n",
        "plt.xticks(range(len(c_names)))\n",
        "plt.yticks(range(len(words)))\n",
        "plt.ylim([len(words) - 1.5, -.5])\n",
        "ax = plt.gca()\n",
        "ax.set_xticklabels(c_names)\n",
        "ax.set_yticklabels(range(1, len(words) + 1))\n",
        "plt.title(\"$V$\")\n",
        "plt.colorbar();"
      ],
      "metadata": {
        "id": "esOxV7Kl0YiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(U[:,1], index=words)"
      ],
      "metadata": {
        "id": "SsdhYtnf0rVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* load an image **bee**, and convert it to black and white.\n",
        "\n",
        "[Singular Value Decomposition of an Image](https://www.frankcleary.com/svdimage/)"
      ],
      "metadata": {
        "id": "BMROfqZpsJ90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "imag_file = '000000039769.jpg'\n",
        "img = Image.open(imag_file)\n",
        "imggray = img.convert('LA')\n",
        "plt.figure(figsize=(9, 6))\n",
        "plt.imshow(imggray);"
      ],
      "metadata": {
        "id": "QIrep2TFrimj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* convert the image data into a numpy matrix, plotting the result to show the data is unchanged"
      ],
      "metadata": {
        "id": "bkNLvvH0sgu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imgmat = np.array(list(imggray.getdata(band=0)), float)\n",
        "imgmat.shape = (imggray.size[1], imggray.size[0])\n",
        "imgmat = np.matrix(imgmat)\n",
        "plt.figure(figsize=(9,6))\n",
        "plt.imshow(imgmat, cmap='gray');"
      ],
      "metadata": {
        "id": "WsOyVGqasSSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " * compute the singular value decomposition"
      ],
      "metadata": {
        "id": "uEW2avRbszRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "U, sigma, V = np.linalg.svd(imgmat)"
      ],
      "metadata": {
        "id": "m-LnTzqjsmww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### reconstruct\n",
        "* Computing an approximation of the image using the first column of  *U*\n",
        "  and first row of  *V*\n",
        "  reproduces the most prominent feature of the image, the light area on top and the dark area on the bottom. The darkness of the arch causes the extra darkness in the middle of the reconstruction. Each column of pixels in this image is a different weighting of the same values,  u⃗ 1\n",
        " :"
      ],
      "metadata": {
        "id": "WROItqpotIte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reconstimg = np.matrix(U[:, :1]) * np.diag(sigma[:1]) * np.matrix(V[:1, :])\n",
        "plt.imshow(reconstimg, cmap='gray');"
      ],
      "metadata": {
        "id": "0s2q4RVts6TR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(2, 5):\n",
        "    reconstimg = np.matrix(U[:, :i]) * np.diag(sigma[:i]) * np.matrix(V[:i, :])\n",
        "    plt.imshow(reconstimg, cmap='gray')\n",
        "    title = \"n = %s\" % i\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "3ph3lMFhs6z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5, 51, 5):\n",
        "    reconstimg = np.matrix(U[:, :i]) * np.diag(sigma[:i]) * np.matrix(V[:i, :])\n",
        "    plt.imshow(reconstimg, cmap='gray')\n",
        "    title = \"n = %s\" % i\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Rc153Hz-t6U4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVD image reconstruction in Python"
      ],
      "metadata": {
        "id": "Pzj03-p_Gbah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "\n",
        "imag_file = '000000039769.jpg'\n",
        "img = Image.open(imag_file)\n",
        "img = np.mean(img, 2)\n",
        "\n",
        "U,s,V = np.linalg.svd(img)\n",
        "\n",
        "n = 10\n",
        "S = np.zeros(np.shape(img))\n",
        "for i in range(0, n):\n",
        "    S[i,i] = s[i]\n",
        "\n",
        "recon_img = U @ S @ V\n",
        "\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "\n",
        "ax[0].imshow(img)\n",
        "ax[0].axis('off')\n",
        "ax[0].set_title('Original')\n",
        "\n",
        "ax[1].imshow(recon_img)\n",
        "ax[1].axis('off')\n",
        "ax[1].set_title(f'Reconstructed n = {n}')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FE-BTt04GaFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://i2.pickpik.com/photos/900/201/265/korea-seoul-jongno-city-c00898e0e8f0998492a96e0c987a672e.jpg"
      ],
      "metadata": {
        "id": "Dzk8JymYxpz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image, ImageFile\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "img = Image.open('korea-seoul-jongno-city-c00898e0e8f0998492a96e0c987a672e.jpg')\n",
        "plt.imshow(img) # 원본 이미지\n",
        "\n",
        "img = img.convert('LA')\n",
        "plt.imshow(img) # 회색빛으로 변환"
      ],
      "metadata": {
        "id": "WaUfTUXUxavx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img.size"
      ],
      "metadata": {
        "id": "qPW9chsHKwsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pix_mat = np.array(img)"
      ],
      "metadata": {
        "id": "FypYTkQLNZZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pix_mat.shape"
      ],
      "metadata": {
        "id": "a184XcpUNhEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Generate a random matrix and send an image safely masked with the matrix  \n"
      ],
      "metadata": {
        "id": "c36muaIfPv3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "rand_vector = np.random.randint(0, 255, size=(img.height,img.width,2))"
      ],
      "metadata": {
        "id": "UWWsHS8uLKdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_hat =np.bitwise_xor(img, rand_vector)\n",
        "im = Image.fromarray(np.uint8(img_hat))"
      ],
      "metadata": {
        "id": "GJ8d0_T4LKdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(im) # 원본 이미지"
      ],
      "metadata": {
        "id": "6nyp8j7-PRjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img =np.bitwise_xor(img_hat, rand_vector)\n",
        "im = Image.fromarray(np.uint8(img))"
      ],
      "metadata": {
        "id": "8MCP2iIaN4s2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(im) # 원본 이미지\n"
      ],
      "metadata": {
        "id": "Nnla9lNAOG69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "img = Image.open('korea-seoul-jongno-city-c00898e0e8f0998492a96e0c987a672e.jpg')\n",
        "\n",
        "img_np = np.array(list(img.getdata(band=0)), float)\n",
        "img_np.shape = (img.size[1], img.size[0])\n",
        "img_np = np.matrix(img_np)"
      ],
      "metadata": {
        "id": "VHLsGlivx8sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(img_np.shape) # 이미지 행렬\n",
        "# (2570, 3854)"
      ],
      "metadata": {
        "id": "G8lrickMx7H-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "U, S, V = np.linalg.svd(img_np)\n",
        "img_svd = np.matrix(U[:, :1]) * np.diag(S[:1]) * np.matrix(V[:1, :])\n",
        "plt.imshow(img_svd, cmap='gray')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JW-nTMzByrRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in [2, 4, 8, 16, 32, 64, 128]:\n",
        "    img_svd = np.matrix(U[:, :k] * np.diag(S[:k]) * np.matrix(V[:k, :]))\n",
        "    plt.imshow(img_svd, cmap='gray')\n",
        "    title = \"SVD k = %s\" % k\n",
        "    plt.title(title)\n",
        "    plt.show() # SVD 결과 이미지\n",
        "\n",
        "plt.imshow(img_np, cmap='gray')\n",
        "plt.title('Original Image')\n",
        "plt.show() # 원본 이미지"
      ],
      "metadata": {
        "id": "6eYpEuMe04Gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cupy as cp\n",
        "img_gpu = cp.array(img_np)"
      ],
      "metadata": {
        "id": "igKyyrix1NsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cp.__version__"
      ],
      "metadata": {
        "id": "iwKLnMpl9fN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "U, S, V = np.linalg.svd(img_np)"
      ],
      "metadata": {
        "id": "HdQif7aq65dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "(U, S, V)  = cp.linalg.svd(img_gpu)"
      ],
      "metadata": {
        "id": "uh2kmeAR6jOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "U = cp.asnumpy(U)\n",
        "S = cp.asnumpy(S)\n",
        "V = cp.asnumpy(V)"
      ],
      "metadata": {
        "id": "M_Ph1Q3w5JaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_svd = np.matrix(U[:, :1]) * np.diag(S[:1]) * np.matrix(V[:1, :])\n",
        "plt.imshow(img_svd, cmap='gray')\n"
      ],
      "metadata": {
        "id": "VYMGbBMc20-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA\n",
        "Principal Component Analysis or PCA is a dimensionality reduction technique for data sets with many features or dimensions. It uses linear algebra to determine the most important features of a dataset.\n",
        "\n",
        "Time complexity is important. For example PCA on a matrix of size n×k, takes 𝑂(𝑘²×𝑛+𝑘³) time. If k is huge (say 10000), PCA can be as slow as a snail. This problem exists with many other Dimension reduction methods as well.\n"
      ],
      "metadata": {
        "id": "mk3QJx9HGaGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA dimention reduction\n",
        "# https://dataknowsall.com/blog/imagepca.html\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "image_raw = Image.open('korea-seoul-jongno-city-c00898e0e8f0998492a96e0c987a672e.jpg')\n",
        "image_bw = image_raw.convert('L')\n",
        "image_bw = np.array(image_bw)\n",
        "\n",
        "k=50\n",
        "pca = PCA(n_components=k)\n",
        "\n",
        "pca.fit(image_bw)\n",
        "trans_pca= pca.transform(image_bw)\n"
      ],
      "metadata": {
        "id": "PeMOlm69EhAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA, IncrementalPCA\n",
        "\n",
        "ipca = IncrementalPCA(n_components=k)\n",
        "image_recon = ipca.inverse_transform(ipca.fit_transform(image_bw))\n",
        "\n",
        "# Plotting the reconstructed image\n",
        "plt.figure(figsize=[12,8])\n",
        "plt.imshow(image_recon,cmap = plt.cm.gray)"
      ],
      "metadata": {
        "id": "IqUhjOXKBcYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_at_k(k):\n",
        "    ipca = IncrementalPCA(n_components=k)\n",
        "    image_recon = ipca.inverse_transform(ipca.fit_transform(image_bw))\n",
        "    plt.imshow(image_recon,cmap = plt.cm.gray)\n",
        "\n",
        "ks = [10, 25, 50, 100, 150, 250]\n",
        "\n",
        "plt.figure(figsize=[15,9])\n",
        "\n",
        "for i in range(6):\n",
        "    plt.subplot(2,3,i+1)\n",
        "    plot_at_k(ks[i])\n",
        "    plt.title(\"Components: \"+str(ks[i]))\n",
        "\n",
        "plt.subplots_adjust(wspace=0.2, hspace=0.0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "92xvRKaRB8U1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import module\n",
        "import requests\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# assign and open image\n",
        "url = 'https://media.geeksforgeeks.org/wp-content/cdn-uploads/20210401173418/Webp-compressed.jpg'\n",
        "response = requests.get(url, stream=True)\n",
        "\n",
        "with open('image.png', 'wb') as f:\n",
        "\tf.write(response.content)\n",
        "\n",
        "img = cv2.imread('image.png')\n",
        "\n",
        "# Converting the image into gray scale for faster\n",
        "# computation.\n",
        "gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Calculating the SVD\n",
        "u, s, v = np.linalg.svd(gray_image, full_matrices=False)\n",
        "\n",
        "# inspect shapes of the matrices\n",
        "print(f'u.shape:{u.shape},s.shape:{s.shape},v.shape:{v.shape}')\n"
      ],
      "metadata": {
        "id": "P3wvZKOmHFVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import module\n",
        "import seaborn as sns\n",
        "\n",
        "var_explained = np.round(s**2/np.sum(s**2), decimals=6)\n",
        "\n",
        "# Variance explained top Singular vectors\n",
        "print(f'variance Explained by Top 20 singular values:\\n{var_explained[0:20]}')\n",
        "\n",
        "sns.barplot(x=list(range(1, 21)),\n",
        "\t\t\ty=var_explained[0:20], color=\"dodgerblue\")\n",
        "\n",
        "plt.title('Variance Explained Graph')\n",
        "plt.xlabel('Singular Vector', fontsize=16)\n",
        "plt.ylabel('Variance Explained', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uHBVWZsDHWOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot images with different number of components\n",
        "comps = [3648, 1, 5, 10, 15, 20]\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "for i in range(len(comps)):\n",
        "    low_rank = u[:, :comps[i]] @ np.diag(s[:comps[i]]) @ v[:comps[i], :]\n",
        "\n",
        "    if(i == 0):\n",
        "        plt.subplot(2, 3, i+1),\n",
        "        plt.imshow(low_rank, cmap='gray'),\n",
        "        plt.title(f'Actual Image with n_components = {comps[i]}')\n",
        "\n",
        "    else:\n",
        "        plt.subplot(2, 3, i+1),\n",
        "        plt.imshow(low_rank, cmap='gray'),\n",
        "        plt.title(f'n_components = {comps[i]}')"
      ],
      "metadata": {
        "id": "DQw3FbZtHcf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random projection"
      ],
      "metadata": {
        "id": "tgJgkxWgTiLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
        "import numpy as np\n",
        "\n",
        "m, ε = 5_000, 0.1\n",
        "d = johnson_lindenstrauss_min_dim(m, eps=ε)\n",
        "d"
      ],
      "metadata": {
        "id": "J9QUwo-KTgtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extra code – show the equation computed by johnson_lindenstrauss_min_dim\n",
        "d = int(4 * np.log(m) / (ε ** 2 / 2 - ε ** 3 / 3))\n",
        "d"
      ],
      "metadata": {
        "id": "JzprMyNdTqD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 20_000\n",
        "np.random.seed(42)\n",
        "P = np.random.randn(d, n) / np.sqrt(d)  # std dev = square root of variance\n",
        "\n",
        "X = np.random.randn(m, n)  # generate a fake dataset\n",
        "X_reduced = X @ P.T"
      ],
      "metadata": {
        "id": "6J0ojDX3TzbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.random_projection import GaussianRandomProjection\n",
        "\n",
        "gaussian_rnd_proj = GaussianRandomProjection(eps=ε, random_state=42)\n",
        "X_reduced = gaussian_rnd_proj.fit_transform(X)  # same result as above"
      ],
      "metadata": {
        "id": "9FeelvD8T8zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "components_pinv = np.linalg.pinv(gaussian_rnd_proj.components_)\n",
        "X_recovered = X_reduced @ components_pinv.T"
      ],
      "metadata": {
        "id": "5qt9DIGbUKsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extra code – performance comparison between Gaussian and Sparse RP\n",
        "\n",
        "from sklearn.random_projection import SparseRandomProjection\n",
        "\n",
        "print(\"GaussianRandomProjection fit\")\n",
        "%timeit GaussianRandomProjection(random_state=42).fit(X)\n",
        "print(\"SparseRandomProjection fit\")\n",
        "%timeit SparseRandomProjection(random_state=42).fit(X)\n",
        "\n",
        "gaussian_rnd_proj = GaussianRandomProjection(random_state=42).fit(X)\n",
        "sparse_rnd_proj = SparseRandomProjection(random_state=42).fit(X)\n",
        "print(\"GaussianRandomProjection transform\")\n",
        "%timeit gaussian_rnd_proj.transform(X)\n",
        "print(\"SparseRandomProjection transform\")\n",
        "%timeit sparse_rnd_proj.transform(X)"
      ],
      "metadata": {
        "id": "oAn6205BUYOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 이미지 SVD 적용"
      ],
      "metadata": {
        "id": "0u6ghJuN8SdJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgVLmLn6aRPb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.linalg import svd, norm\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import h5py\n",
        "import os\n",
        "# define class labels\n",
        "labels = {\n",
        "    0: \"0\",\n",
        "    1: \"1\",\n",
        "    2: \"2\",\n",
        "    3: \"3\",\n",
        "    4: \"4\",\n",
        "    5: \"5\",\n",
        "    6: \"6\",\n",
        "    7: \"7\",\n",
        "    8: \"8\",\n",
        "    9: \"9\"\n",
        "}\n",
        "# load the dataset\n",
        "with h5py.File(os.path.join(os.getcwd(), 'usps.h5'), 'r') as hf:\n",
        "        train = hf.get('train')\n",
        "        test = hf.get('test')\n",
        "        x_train = pd.DataFrame(train.get('data')[:]).T\n",
        "        y_train = pd.DataFrame(train.get('target')[:]).T\n",
        "        x_test = pd.DataFrame(test.get('data')[:]).T\n",
        "        y_test = pd.DataFrame(test.get('target')[:]).T\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "digit_image=x_train[0]\n",
        "plt.imshow(digit_image.to_numpy().reshape(16,16),cmap='binary')"
      ],
      "metadata": {
        "id": "LUO5asi6a8Zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha_matrices={}\n",
        "for i in range(10):\n",
        "    alpha_matrices.update({\"A\"+str(i):x_train.loc[:,list(y_train.loc[0,:]==i)]})\n",
        "print(alpha_matrices['A0'].shape)\n",
        "#(256, 1194)"
      ],
      "metadata": {
        "id": "A-izxojia9Hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "left_singular={}\n",
        "singular_matix={}\n",
        "right_singular={}\n",
        "for i in range(10):\n",
        "    u, s, v_t = svd(alpha_matrices['A'+str(i)], full_matrices=False)\n",
        "    left_singular.update({\"u\"+str(i):u})\n",
        "    singular_matix.update({\"s\"+str(i):s})\n",
        "    right_singular.update({\"v_t\"+str(i):v_t})\n",
        "print(left_singular['u0'].shape)\n",
        "#(256, 256)"
      ],
      "metadata": {
        "id": "XEVOXat1bauV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#left_singular[‘u3’]\n",
        "#right_singular[‘s3]\n",
        "#singular_matix[‘v_t3]\n",
        "plt.figure(figsize=(20,10))\n",
        "columns = 5\n",
        "for i in range(10):\n",
        "   plt.subplot(10/ columns + 1, columns, i + 1)\n",
        "   plt.imshow(left_singular[\"u3\"][:,i].reshape(16,16),cmap='binary')"
      ],
      "metadata": {
        "id": "CiaA8sMEbroK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (9, 6))\n",
        "plt.plot(singular_matix['s3'], color='coral', marker='o')\n",
        "plt.title('Singular values for digit $3$',fontsize=15,weight=\"bold\",pad=20)\n",
        "plt.ylabel('Singular values' ,fontsize=15)\n",
        "plt.yscale(\"log\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6_Zpt1b4b5XN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "I = np.eye(x_test.shape[0])\n",
        "kappas=np.arange(5,21)\n",
        "len_test=x_test.shape[1]\n",
        "predictions=np.empty((y_test.shape[1],0), dtype = int)\n",
        "for t in list(kappas):\n",
        "    prediction = []\n",
        "    for i in range(len_test):\n",
        "        residuals = []\n",
        "        for j in range(10):\n",
        "            u=left_singular[\"u\"+str(j)][:,0:t]\n",
        "            res=norm( np.dot(I-np.dot(u,u.T), x_test[i]  ))\n",
        "            residuals.append(res)\n",
        "        index_min = np.argmin(residuals)\n",
        "        prediction.append(index_min)\n",
        "\n",
        "    prediction=np.array(prediction)\n",
        "    predictions=np.hstack((predictions,prediction.reshape(-1,1)))\n"
      ],
      "metadata": {
        "id": "iufUiccPcVwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores=[]\n",
        "thresholds = []\n",
        "for i in range(len(kappas)):\n",
        "    score=accuracy_score(y_test.loc[0,:],predictions[:,i])\n",
        "    thresholds.append (y_test.loc[0,:][i])\n",
        "    scores.append(score)\n"
      ],
      "metadata": {
        "id": "3M5HbV98OLL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores"
      ],
      "metadata": {
        "id": "HzUj5lnfO2oL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data={\"Number of basis vectors\":list(thresholds), \"accuracy_score\":scores}\n",
        "df=pd.DataFrame(data).set_index(\"Number of basis vectors\")"
      ],
      "metadata": {
        "id": "hi1DPds2Oy7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth',12)\n",
        "confusion_matrix_df = pd.DataFrame( confusion_matrix(y_test.loc[0,:],predictions[:,7]) )\n",
        "confusion_matrix_df = confusion_matrix_df.rename(columns = labels, index = labels)\n",
        "confusion_matrix_df"
      ],
      "metadata": {
        "id": "PAZUBw8KcjYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test.loc[0,:],predictions[:,7]))"
      ],
      "metadata": {
        "id": "iPkPxEcicmsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "misclassified = np.where(y_test.loc[0,:] != predictions[:,7])\n",
        "plt.figure(figsize=(20,10))\n",
        "columns = 5\n",
        "for i in range(2,12):\n",
        "    misclassified_id=misclassified[0][i]\n",
        "    image=x_test[misclassified_id]\n",
        "\n",
        "    plt.subplot(10/ columns + 1, columns, i-1)\n",
        "    plt.imshow(image.to_numpy().reshape(16,16),cmap='binary')\n",
        "    plt.title(\"True label:\"+str(y_test.loc[0,misclassified_id]) + '\\n'+ \"Predicted label:\"+str(predictions[misclassified_id,12]))"
      ],
      "metadata": {
        "id": "uQedm4Y4cs6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LU factorization"
      ],
      "metadata": {
        "id": "VjWiLOqmnNKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def lu_factorization(A):\n",
        "    \"\"\"\n",
        "    Performs LU factorization of a square matrix A.\n",
        "\n",
        "    Args:\n",
        "        A (numpy.ndarray): The square matrix to factorize.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing three numpy.ndarrays:\n",
        "               - L: The lower triangular matrix.\n",
        "               - U: The upper triangular matrix.\n",
        "               - P: The permutation matrix (identity if no row swaps are needed).\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the input matrix A is not square.\n",
        "    \"\"\"\n",
        "    n = A.shape[0]\n",
        "    if A.shape[0] != A.shape[1]:\n",
        "        raise ValueError(\"Input matrix must be square.\")\n",
        "\n",
        "    L = np.identity(n)\n",
        "    U = np.array(A, dtype=float)  # Create a copy to avoid modifying the original\n",
        "    P = np.identity(n)\n",
        "\n",
        "    for k in range(n):\n",
        "        # Find the pivot element (largest absolute value in the current column)\n",
        "        pivot_row = np.argmax(np.abs(U[k:, k])) + k\n",
        "\n",
        "        # If pivot is zero, matrix is singular (cannot continue without division by zero)\n",
        "        if U[pivot_row, k] == 0:\n",
        "            raise ValueError(\"Matrix is singular, LU factorization cannot be completed.\")\n",
        "\n",
        "        # Swap rows in U and P\n",
        "        U[[k, pivot_row]] = U[[pivot_row, k]]\n",
        "        P[[k, pivot_row]] = P[[pivot_row, k]]\n",
        "\n",
        "        # Swap corresponding rows in L (below the diagonal)\n",
        "        if k > 0:\n",
        "            L[[k, pivot_row], :k] = L[[pivot_row, k], :k]\n",
        "\n",
        "        # Eliminate elements below the pivot\n",
        "        for i in range(k + 1, n):\n",
        "            factor = U[i, k] / U[k, k]\n",
        "            L[i, k] = factor\n",
        "            U[i, k:] = U[i, k:] - factor * U[k, k:]\n",
        "\n",
        "    return L, U, P\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Example usage:\n",
        "    A = np.array([[2, 1, 1],\n",
        "                  [4, -6, 0],\n",
        "                  [-2, 7, 2]])\n",
        "\n",
        "    try:\n",
        "        L, U, P = lu_factorization(A)\n",
        "        print(\"Original Matrix A:\\n\", A)\n",
        "        print(\"\\nLower Triangular Matrix L:\\n\", L)\n",
        "        print(\"\\nUpper Triangular Matrix U:\\n\", U)\n",
        "        print(\"\\nPermutation Matrix P:\\n\", P)\n",
        "\n",
        "        # Verify the factorization: P * A = L * U\n",
        "        import numpy.linalg\n",
        "        print(\"\\nVerification (P @ A):\\n\", P @ A)\n",
        "        print(\"\\nVerification (L @ U):\\n\", L @ U)\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "    print(\"\\n--- Example with row swap ---\")\n",
        "    A_swap = np.array([[0, 1],\n",
        "                       [1, 0]])\n",
        "    try:\n",
        "        L_swap, U_swap, P_swap = lu_factorization(A_swap)\n",
        "        print(\"Original Matrix A_swap:\\n\", A_swap)\n",
        "        print(\"\\nLower Triangular Matrix L_swap:\\n\", L_swap)\n",
        "        print(\"\\nUpper Triangular Matrix U_swap:\\n\", U_swap)\n",
        "        print(\"\\nPermutation Matrix P_swap:\\n\", P_swap)\n",
        "        print(\"\\nVerification (P_swap @ A_swap):\\n\", P_swap @ A_swap)\n",
        "        print(\"\\nVerification (L_swap @ U_swap):\\n\", L_swap @ U_swap)\n",
        "    except ValueError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "    print(\"\\n--- Example of a singular matrix ---\")\n",
        "    A_singular = np.array([[1, 1],\n",
        "                           [2, 2]])\n",
        "    try:\n",
        "        L_singular, U_singular, P_singular = lu_factorization(A_singular)\n",
        "        print(\"Original Matrix A_singular:\\n\", A_singular)\n",
        "        print(\"\\nLower Triangular Matrix L_singular:\\n\", L_singular)\n",
        "        print(\"\\nUpper Triangular Matrix U_singular:\\n\", U_singular)\n",
        "        print(\"\\nPermutation Matrix P_singular:\\n\", P_singular)\n",
        "        print(\"\\nVerification (P_singular @ A_singular):\\n\", P_singular @ A_singular)\n",
        "        print(\"\\nVerification (L_singular @ U_singular):\\n\", L_singular @ U_singular)\n",
        "    except ValueError as e:\n",
        "        print(f\"Error: {e}\")"
      ],
      "metadata": {
        "id": "rMDorcKwnQls"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}